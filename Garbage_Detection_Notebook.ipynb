{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Garbage Detection Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalisgalanis/garbage-detection-internship/blob/main/Garbage_Detection_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Uun_H2emM9"
      },
      "source": [
        "# **A. Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbdUmIvyfBda"
      },
      "source": [
        "## Acquiring the TACO dataset\n",
        "TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labeled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. Currently, images are hosted on\n",
        "Flickr and we have a server that is collecting more images and annotations.\n",
        "\n",
        "Get more information [here](https://github.com/pedropro/TACO)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BlwQmP2X0lR"
      },
      "source": [
        "# 0. Clear Data\n",
        "import os, shutil\n",
        "if os.path.exists('sample_data/'): shutil.rmtree('sample_data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Q_zfK_kCBe"
      },
      "source": [
        "# 2.1 Clone & Download dataset\n",
        "!git clone https://github.com/pedropro/TACO\n",
        "%cd /content/TACO\n",
        "!python3 download.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijfVDNzYXnsI"
      },
      "source": [
        "# 2.2 OR Upload TACO.zip with 2-3 images (for simplicity's sake)\n",
        "import os, shutil\n",
        "\n",
        "%cd /content\n",
        "if os.path.exists('/content/TACO'): shutil.rmtree('/content/TACO')\n",
        "!unzip /content/TACO.zip\n",
        "if os.path.exists('/content/TACO.zip'): os.remove('TACO.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yc4soYE91wI"
      },
      "source": [
        "## Organize files & folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebg1pYCR98c-"
      },
      "source": [
        "# 3. Move all images from multiple batch folders to a new single folder.\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "root_path = 'TACO'\n",
        "data_path = os.path.join(root_path, 'data')\n",
        "image_path = os.path.join(data_path, 'images')\n",
        "\n",
        "# Create new folder 'images'\n",
        "if os.path.exists(image_path):\n",
        "    shutil.rmtree(image_path)\n",
        "os.mkdir(image_path)\n",
        "\n",
        "for batch in os.listdir(data_path):\n",
        "    if os.path.isdir(data_path) and 'batch' in batch:\n",
        "        for image in os.listdir(os.path.join(data_path, batch)):\n",
        "            shutil.move(os.path.join(data_path, batch, image), os.path.join(image_path, batch + '_' + image))\n",
        "        shutil.rmtree(os.path.join(data_path, batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQSu6D90fIvg"
      },
      "source": [
        "# 4. Split dataset into train and val\n",
        "import random\n",
        "import math\n",
        "\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "json_filename = 'annotations.json'\n",
        "train_val_ratio = 0.9\n",
        "\n",
        "images = os.listdir(image_path)\n",
        "random.shuffle(images)\n",
        "train_images = images[0 : math.floor(train_val_ratio * len(images))]\n",
        "val_images = images[math.floor(train_val_ratio * len(images)):]\n",
        "\n",
        "# Create train & val folders\n",
        "if os.path.exists(train_path):\n",
        "    shutil.rmtree(train_path)\n",
        "os.mkdir(train_path)\n",
        "\n",
        "if os.path.exists(val_path):\n",
        "    shutil.rmtree(val_path)\n",
        "os.mkdir(val_path)\n",
        "\n",
        "for train_image in train_images:\n",
        "    shutil.move(os.path.join(image_path, train_image), os.path.join(train_path, train_image))\n",
        "\n",
        "for val_image in val_images:\n",
        "    shutil.move(os.path.join(image_path, val_image), os.path.join(val_path, val_image))\n",
        "\n",
        "shutil.rmtree(image_path)\n",
        "\n",
        "# Copy annotations to both splits\n",
        "shutil.copy(os.path.join(data_path, json_filename), os.path.join(train_path, json_filename))\n",
        "shutil.copy(os.path.join(data_path, json_filename), os.path.join(val_path, json_filename))\n",
        "os.remove(os.path.join(data_path, json_filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_FJAmU7tUZr"
      },
      "source": [
        "# **B. Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_48NuOF2a5ZH"
      },
      "source": [
        "### Setting up training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU27Sf-Wb2k3",
        "outputId": "f34b5189-8f51-4a0b-c2fb-d8efa0e6e341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 1. Clone Mask_RCNN repo\n",
        "import os, shutil\n",
        "\n",
        "model_path = '/content/Mask_RCNN'\n",
        "if os.path.exists(model_path): shutil.rmtree(model_path)\n",
        "\n",
        "!git clone https://github.com/matterport/Mask_RCNN /content/Mask_RCNN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Mask_RCNN/'\n",
            "/content\n",
            "Cloning into '/content/Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\n",
            "Receiving objects: 100% (956/956), 111.85 MiB | 40.34 MiB/s, done.\n",
            "Resolving deltas: 100% (563/563), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPrkZqaoo4xF",
        "outputId": "a5f7f2b6-ffd6-45f6-f715-8c33c15f6f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 2. Configure Dataset\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import skimage.io\n",
        "\n",
        "\n",
        "%cd /content/Mask_RCNN\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"/TACO\") \n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "json_filename = 'annotations.json'\n",
        "\n",
        "from Mask_RCNN.mrcnn.config import Config\n",
        "from Mask_RCNN.mrcnn import model as modellib, utils\n",
        "\n",
        "# Path to trained weights file\n",
        "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "\n",
        "# Directory to save logs and model checkpoints, if not provided\n",
        "# through the command line argument --logs\n",
        "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "class GarbageConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"garbage\"\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 60  # Background + garbage categories\n",
        "\n",
        "    # Number of training steps per epoch\n",
        "    STEPS_PER_EPOCH = 100\n",
        "\n",
        "    # Skip detections with < 90% confidence\n",
        "    DETECTION_MIN_CONFIDENCE = 0.9\n",
        "\n",
        "class GarbageDataset(utils.Dataset):\n",
        "\n",
        "    def load_garbage(self, dataset_dir, subset):\n",
        "        # Read JSON file\n",
        "        json_dict = json.load(open(os.path.join(train_path, json_filename)))\n",
        "        for category in json_dict['categories']:\n",
        "            # Add every class (60 categories)\n",
        "            self.add_class(category['supercategory'], category['id'], category['name'])\n",
        "\n",
        "        # Train or validation dataset?\n",
        "        assert subset in [\"train\", \"val\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "\n",
        "        images_dict = json_dict['images']\n",
        "        annotations = json_dict['annotations']\n",
        "\n",
        "        for image in images_dict:\n",
        "            if (image['file_name'].rpartition('/')[0] + '_' + image['file_name'].rpartition('/')[2]) in os.listdir(dataset_dir):\n",
        "                print('Image',image['file_name'], 'found!')\n",
        "                polygons = []\n",
        "                for ann in annotations:\n",
        "                    if ann['image_id'] == image['id']:\n",
        "                        polygons.append(ann['segmentation'][0])\n",
        "\n",
        "                image_path = os.path.join(dataset_dir, (image['file_name'].rpartition('/')[0] + '_' + image['file_name'].rpartition('/')[2]))\n",
        "                image = skimage.io.imread(image_path)\n",
        "                height, width = image.shape[:2]\n",
        "\n",
        "                print(image_path, height, width)\n",
        "            #else:\n",
        "            #    print('Image',image['file_name'], 'not found!')\n",
        "\n",
        "            self.add_image(\n",
        "                \"garbage\",\n",
        "                image_id = image['file_name'].rpartition('/'),  # use file name as a unique image id\n",
        "                path = image_path,\n",
        "                width = width, height = height,\n",
        "                polygons = polygons)\n",
        "\n",
        "    def train(model):\n",
        "        # Training dataset.\n",
        "        dataset_train = GarbageDataset()\n",
        "        dataset_train.load_garbage(args.dataset, \"train\")\n",
        "        dataset_train.prepare()\n",
        "\n",
        "        # Validation dataset\n",
        "        dataset_val = GarbageDataset()\n",
        "        dataset_val.load_garbage(args.dataset, \"val\")\n",
        "        dataset_val.prepare()\n",
        "\n",
        "        print(\"Training network heads\")\n",
        "        model.train(dataset_train, dataset_val,\n",
        "                    learning_rate=config.LEARNING_RATE,\n",
        "                    epochs=30,\n",
        "                    layers='heads')       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mask_RCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaJSYn08K81d"
      },
      "source": [
        "# TODO\n",
        "def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "        Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        # If not a balloon dataset image, delegate to parent class.\n",
        "        image_info = self.image_info[image_id]\n",
        "        if image_info[\"source\"] != \"balloon\":\n",
        "            return super(self.__class__, self).load_mask(image_id)\n",
        "\n",
        "        # Convert polygons to a bitmap mask of shape\n",
        "        # [height, width, instance_count]\n",
        "        info = self.image_info[image_id]\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
        "                        dtype=np.uint8)\n",
        "        for i, p in enumerate(info[\"polygons\"]):\n",
        "            # Get indexes of pixels inside the polygon and set them to 1\n",
        "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
        "            mask[rr, cc, i] = 1\n",
        "\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID only, we return an array of 1s\n",
        "        return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
        "\n",
        "def image_reference(self, image_id):\n",
        "    \"\"\"Return the path of the image.\"\"\"\n",
        "    info = self.image_info[image_id]\n",
        "    if info[\"source\"] == \"balloon\":\n",
        "        return info[\"path\"]\n",
        "    else:\n",
        "        super(self.__class__, self).image_reference(image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoMPHmGqfQna"
      },
      "source": [
        "%cd /content/\n",
        "!ls\n",
        "\n",
        "config = GarbageConfig()\n",
        "#config.display()\n",
        "\n",
        "\n",
        "folders = ['train', 'val']\n",
        "for folder in folders:\n",
        "    for image in os.listdir(os.path.join(ROOT_DIR, 'data', folder)):\n",
        "        # Resize Images\n",
        "        image = skimage.io.imread(os.path.join(ROOT_DIR, 'data', folder, image))\n",
        "\n",
        "        original_shape = image.shape\n",
        "        image, window, scale, padding, _ = utils.resize_image(\n",
        "            image, \n",
        "            min_dim=config.IMAGE_MIN_DIM, \n",
        "            max_dim=config.IMAGE_MAX_DIM,\n",
        "            mode=config.IMAGE_RESIZE_MODE)\n",
        "        skimage.io.imsave(os.path.join(ROOT_DIR, 'data', folder), image)\n",
        "\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)\n",
        "\n",
        "weights_path = COCO_WEIGHTS_PATH\n",
        "# Download weights file\n",
        "if not os.path.exists(weights_path):\n",
        "    utils.download_trained_weights(weights_path)\n",
        "\n",
        "\n",
        "print(\"Loading weights \", weights_path)\n",
        "model.load_weights(weights_path, by_name=True, exclude=[\n",
        "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
        "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "train(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHWuR1RBbxXg"
      },
      "source": [
        "### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thcKxkqEoxaN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx8rPFa6tXFk"
      },
      "source": [
        "# **C. Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW9IplmtaFf2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}